{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Experimentation solving MNIST\n",
    "\n",
    "### Objective\n",
    "In this notebook, we will learn to experiment solving MNIST dataset using several different learning algorithm e.g. Logistic Regression, Random Forest, Multi Layer Perceptron (MLP), and Convolutional Neural Network (CNN)\n",
    "\n",
    "### Problem\n",
    "Given 28x28 pixels of handwritten digit, predict the number (0-9), hence this is multi-class classification problem.\n",
    "\n",
    "### Evaluation Metrics\n",
    "- Accuracy\n",
    "- F1 score\n",
    "- Precision\n",
    "- Recall\n",
    "- Time taken for training\n",
    "- Time taken for predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential # for bulding MLP and CNN\n",
    "import numpy as np\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# evaluating performance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# mlflow tracking\n",
    "import mlflow\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# others\n",
    "import random\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28), (60000,), (10000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dataaset could be downloaded using Keras API\n",
    "# It consists 28x28 greyscale images of 10 digits, 60k samples for training set, and 10k samples for test set\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n",
      "{0: 0.099, 1: 0.112, 2: 0.099, 3: 0.102, 4: 0.097, 5: 0.09, 6: 0.099, 7: 0.104, 8: 0.098, 9: 0.099}\n"
     ]
    }
   ],
   "source": [
    "# Number of samples for each class in training set\n",
    "unique, count = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# number of samples per class\n",
    "print(dict(zip(unique, count)))\n",
    "\n",
    "# proportion of each class\n",
    "print(dict(zip(unique, np.round(count / count.sum(),3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}\n",
      "{0: 0.098, 1: 0.114, 2: 0.103, 3: 0.101, 4: 0.098, 5: 0.089, 6: 0.096, 7: 0.103, 8: 0.097, 9: 0.101}\n"
     ]
    }
   ],
   "source": [
    "# Number of samples for each class in test set\n",
    "unique, count = np.unique(y_test, return_counts=True)\n",
    "\n",
    "# number of samples per class\n",
    "print(dict(zip(unique, count)))\n",
    "\n",
    "# proportion of each class\n",
    "print(dict(zip(unique, np.round(count / count.sum(),3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd81059e9b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show example of image\n",
    "\n",
    "# get random index\n",
    "n = random.randint(0,59999)\n",
    "\n",
    "# print label & show image\n",
    "print(\"Label: %s\" % (y_train[n]))\n",
    "plt.imshow(X_train[n,:], interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "In this section, we will:\n",
    "1. Create two types of features space: flatten and as-is.\n",
    "2. Normalize the value previously ranging from 0-255 to 0-1 to fasten the convergence time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examlple value on pixels before normalization \n",
      "[  0   0   0  46 245 163   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 198 254  56   0   0   0   0   0   0]\n",
      "Examlple value on pixels after normalization \n",
      "[0.         0.         0.         0.18039216 0.96078431 0.63921569\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.77647059 0.99607843 0.21960784 0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data\n",
    "\n",
    "# print before normalize\n",
    "print(\"Examlple value on pixels before normalization \") # the higher the darker\n",
    "print(X_train[2,10,:])\n",
    "\n",
    "# normalize\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# print after normalize\n",
    "print(\"Examlple value on pixels after normalization \")\n",
    "print(X_train[2,10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000, 28, 28, 1), (10000, 784), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create flatten feature space\n",
    "\n",
    "# reshape from 28x28 to 784\n",
    "X_train_flat = X_train.reshape(-1,784) \n",
    "X_test_flat = X_test.reshape(-1, 784)\n",
    "\n",
    "# reshape the 28x28 to 28x28x1 for input CNN\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Check the final shape\n",
    "X_train_flat.shape, X_train.shape, X_test_flat.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(y_test, y_pred, dataset):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "    precision = precision_score(y_test, y_pred, average='micro')\n",
    "    recall = recall_score(y_test, y_pred, average='micro')\n",
    "    \n",
    "    print(\"\\nPerformance metrics on %s set\" % dataset)    \n",
    "    print(\"Accuracy on %s set: %f\" % (dataset, accuracy))\n",
    "    print(\"F1 macro on %s set: %f\" % (dataset, f1_macro))\n",
    "    print(\"F1 micro on %s set: %f\" % (dataset, f1_micro))\n",
    "    print(\"Precision micro on %s set: %f\" % (dataset, precision))\n",
    "    print(\"Recall micro on %s set: %f\" % (dataset, recall))\n",
    "    \n",
    "    return accuracy, precision, recall, f1_micro, f1_macro\n",
    "\n",
    "def print_timing(training_time, prediction_time):\n",
    "    print(\"\\nTraining time: %f s\" % training_time)\n",
    "    print(\"Prediction time: %f s\" % prediction_time)\n",
    "    \n",
    "def track_performance_metrics(accuracy, precision, recall, f1_micro, f1_macro,\n",
    "                              accuracy_train, precision_train, recall_train, f1_micro_train, f1_macro_train,\n",
    "                              training_time, prediction_time):\n",
    "    # performance test set\n",
    "    mlflow.log_metric('accuracy', accuracy)\n",
    "    mlflow.log_metric('f1_macro', f1_macro)\n",
    "    mlflow.log_metric('f1_micro', f1_micro)\n",
    "    mlflow.log_metric('precision', precision)\n",
    "    mlflow.log_metric('recall', recall)\n",
    "\n",
    "    # performance train set\n",
    "    mlflow.log_metric('accuracy_train', accuracy_train)\n",
    "    mlflow.log_metric('f1_macro_train', f1_macro_train)\n",
    "    mlflow.log_metric('f1_micro_train', f1_micro_train)\n",
    "    mlflow.log_metric('precision_train', precision_train)\n",
    "    mlflow.log_metric('recall_train', recall_train)\n",
    "    \n",
    "    mlflow.log_metric('training_time', training_time)\n",
    "    mlflow.log_metric('prediction_time', prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment Id in MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create experiment id for tracking using mlflow\n",
    "exp_id = mlflow.create_experiment(\"solving-mnist1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling: Multi-class Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance metrics on test set\n",
      "Accuracy on test set: 0.925800\n",
      "F1 macro on test set: 0.924792\n",
      "F1 micro on test set: 0.925800\n",
      "Precision micro on test set: 0.925800\n",
      "Recall micro on test set: 0.925800\n",
      "\n",
      "Performance metrics on train set\n",
      "Accuracy on train set: 0.935050\n",
      "F1 macro on train set: 0.934227\n",
      "F1 micro on train set: 0.935050\n",
      "Precision micro on train set: 0.935050\n",
      "Recall micro on train set: 0.935050\n",
      "\n",
      "Training time: 9.372897 s\n",
      "Prediction time: 0.017661 s\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "logreg = LogisticRegression(random_state = 33) # use default hyperparameters\n",
    "\n",
    "with mlflow.start_run(experiment_id = exp_id):\n",
    "    \n",
    "    mlflow.log_param('model', 'logistic_regression')\n",
    "\n",
    "    # training model\n",
    "    start_training = time.time()\n",
    "    logreg.fit(X_train_flat, y_train)\n",
    "    end_training = time.time()\n",
    "\n",
    "    # predict the test set\n",
    "    start_prediction = time.time()\n",
    "    y_pred = logreg.predict(X_test_flat)\n",
    "    end_prediction = time.time()\n",
    "    \n",
    "    # predict the train set\n",
    "    y_pred_train = logreg.predict(X_train_flat)\n",
    "\n",
    "    # Evaluate performance\n",
    "    accuracy, precision, recall, f1_micro, f1_macro = evaluate_performance(y_test, y_pred, 'test')\n",
    "    accuracy_train, precision_train, recall_train, f1_micro_train, f1_macro_train = evaluate_performance(y_train,\n",
    "                                                                                                         y_pred_train,\n",
    "                                                                                                         'train')\n",
    "    training_time = end_training - start_training\n",
    "    prediction_time = end_prediction - start_prediction\n",
    "    print_timing(training_time, prediction_time)\n",
    "    \n",
    "    # Track performance metrics\n",
    "    track_performance_metrics(accuracy, precision, recall, f1_micro, f1_macro,\n",
    "                              accuracy_train, precision_train, recall_train, f1_micro_train, f1_macro_train,\n",
    "                              training_time, prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance metrics on test set\n",
      "Accuracy on test set: 0.968600\n",
      "F1 macro on test set: 0.968310\n",
      "F1 micro on test set: 0.968600\n",
      "Precision micro on test set: 0.968600\n",
      "Recall micro on test set: 0.968600\n",
      "\n",
      "Performance metrics on train set\n",
      "Accuracy on train set: 1.000000\n",
      "F1 macro on train set: 1.000000\n",
      "F1 micro on train set: 1.000000\n",
      "Precision micro on train set: 1.000000\n",
      "Recall micro on train set: 1.000000\n",
      "\n",
      "Training time: 35.693365 s\n",
      "Prediction time: 0.384962 s\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "rfc = RandomForestClassifier(random_state= 33) # use default hyperparameters\n",
    "\n",
    "with mlflow.start_run(experiment_id = exp_id):\n",
    "    \n",
    "    mlflow.log_param('model', 'random_forest')\n",
    "    \n",
    "    # training model\n",
    "    start_training = time.time()\n",
    "    rfc.fit(X_train_flat, y_train)\n",
    "    end_training = time.time()\n",
    "\n",
    "    # predict the test set\n",
    "    start_prediction = time.time()\n",
    "    y_pred = rfc.predict(X_test_flat)\n",
    "    end_prediction = time.time()\n",
    "\n",
    "    # predict the train set\n",
    "    y_pred_train = rfc.predict(X_train_flat)\n",
    "\n",
    "    # Evaluate performance\n",
    "    accuracy, precision, recall, f1_micro, f1_macro = evaluate_performance(y_test, y_pred, 'test')\n",
    "    accuracy_train, precision_train, recall_train, f1_micro_train, f1_macro_train = evaluate_performance(y_train,\n",
    "                                                                                                         y_pred_train,\n",
    "                                                                                                         'train')\n",
    "    training_time = end_training - start_training\n",
    "    prediction_time = end_prediction - start_prediction\n",
    "    print_timing(training_time, prediction_time)\n",
    "    \n",
    "    # Track performance metrics\n",
    "    track_performance_metrics(accuracy, precision, recall, f1_micro, f1_macro,\n",
    "                              accuracy_train, precision_train, recall_train, f1_micro_train, f1_macro_train,\n",
    "                              training_time, prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling: Multi Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 20s 330us/sample - loss: 0.5458 - accuracy: 0.9402\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 19s 319us/sample - loss: 0.2701 - accuracy: 0.9691\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 20s 325us/sample - loss: 0.2027 - accuracy: 0.9762\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 21s 344us/sample - loss: 0.1652 - accuracy: 0.9802\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 19s 322us/sample - loss: 0.1417 - accuracy: 0.9832\n",
      "\n",
      "Performance metrics on test set\n",
      "Accuracy on test set: 0.975000\n",
      "F1 macro on test set: 0.974875\n",
      "F1 micro on test set: 0.975000\n",
      "Precision micro on test set: 0.975000\n",
      "Recall micro on test set: 0.975000\n",
      "\n",
      "Performance metrics on train set\n",
      "Accuracy on train set: 0.985267\n",
      "F1 macro on train set: 0.985232\n",
      "F1 micro on train set: 0.985267\n",
      "Precision micro on train set: 0.985267\n",
      "Recall micro on train set: 0.985267\n",
      "\n",
      "Training time: 98.765327 s\n",
      "Prediction time: 0.790454 s\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "# We will use 1 hidden layers in this model and 384/256 units for each layers\n",
    "# L1 regularization is applied to get faster convergence time in sparse dataset like MNIST\n",
    "\n",
    "mlp_model = keras.Sequential([\n",
    "    keras.layers.Dense(384, activation= 'relu'\n",
    "                       , kernel_initializer= 'he_normal'\n",
    "                       , kernel_regularizer= keras.regularizers.l1(0.00001)\n",
    "                      ),\n",
    "    keras.layers.Dense(256, activation= 'relu'\n",
    "                       , kernel_initializer= 'he_normal'\n",
    "                       , kernel_regularizer= keras.regularizers.l1(0.0001)\n",
    "                      ),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "mlp_model.compile(optimizer= 'adam',\n",
    "                  loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics= ['accuracy'])\n",
    "\n",
    "with mlflow.start_run(experiment_id = exp_id):\n",
    "    \n",
    "    mlflow.log_param('model', 'mlp')\n",
    "\n",
    "    # training model\n",
    "    start_training = time.time()\n",
    "    mlp_model.fit(X_train_flat, y_train, epochs=5)\n",
    "    end_training = time.time()\n",
    "\n",
    "    # predict the test set\n",
    "    start_prediction = time.time()\n",
    "    y_pred_long = mlp_model.predict(X_test_flat)\n",
    "    y_pred = np.argmax(y_pred_long, axis=1)\n",
    "    end_prediction = time.time()\n",
    "\n",
    "    # predict the train set\n",
    "    y_pred_train_long = mlp_model.predict(X_train_flat)\n",
    "    y_pred_train = np.argmax(y_pred_train_long, axis=1)\n",
    "\n",
    "    # Evaluate performance\n",
    "    accuracy, precision, recall, f1_micro, f1_macro = evaluate_performance(y_test, y_pred, 'test')\n",
    "    accuracy_train, precision_train, recall_train, f1_micro_train, f1_macro_train = evaluate_performance(y_train,\n",
    "                                                                                                         y_pred_train,\n",
    "                                                                                                         'train')\n",
    "    training_time = end_training - start_training\n",
    "    prediction_time = end_prediction - start_prediction\n",
    "    print_timing(training_time, prediction_time)\n",
    "    \n",
    "    # Track performance metrics\n",
    "    track_performance_metrics(accuracy, precision, recall, f1_micro, f1_macro,\n",
    "                              accuracy_train, precision_train, recall_train, f1_micro_train, f1_macro_train,\n",
    "                              training_time, prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling: Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 33s 543us/sample - loss: 1.5648 - accuracy: 0.8978\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 29s 487us/sample - loss: 1.4819 - accuracy: 0.9798\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 28s 463us/sample - loss: 1.4792 - accuracy: 0.9820\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 29s 477us/sample - loss: 1.4783 - accuracy: 0.9828\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 30s 495us/sample - loss: 1.4765 - accuracy: 0.9848\n",
      "\n",
      "Performance metrics on test set\n",
      "Accuracy on test set: 0.985300\n",
      "F1 macro on test set: 0.985225\n",
      "F1 micro on test set: 0.985300\n",
      "Precision micro on test set: 0.985300\n",
      "Recall micro on test set: 0.985300\n",
      "\n",
      "Performance metrics on train set\n",
      "Accuracy on train set: 0.985267\n",
      "F1 macro on train set: 0.985232\n",
      "F1 micro on train set: 0.985267\n",
      "Precision micro on train set: 0.985267\n",
      "Recall micro on train set: 0.985267\n",
      "\n",
      "Training time: 147.970940 s\n",
      "Prediction time: 2.224063 s\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "# The model is constructed by several layers\n",
    "# We use relu activation and he initializer to get faster converge time\n",
    "# The first layer is convolutional layer with 16 filter, 3x3 kernel size, 1 stride\n",
    "# The second layer is the same with the first layer. They are supposed to capture feature maps from digit image.\n",
    "# The third layer is pooling with maximum aggregation. It is used to reduce the size of feature maps.\n",
    "# The fourth layer is convolutional layer for the result from maximum pooling.\n",
    "# The next layer is dense neural network with 256 units, before feeding this layer with data,\n",
    "# we need to flatten first the data\n",
    "# The last layer / output layer is dense neural network with 10 units (the same with number of class)\n",
    "\n",
    "cnn_model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1), # shape 28x28x1\n",
    "                        kernel_initializer= 'he_normal'),\n",
    "    keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', \n",
    "                        kernel_initializer= 'he_normal'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(3, 3)),\n",
    "    keras.layers.Conv2D(16, kernel_size=(3, 3), activation='relu', \n",
    "                        kernel_initializer= 'he_normal'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation='relu', \n",
    "                       kernel_initializer= 'he_normal'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "cnn_model.compile(optimizer= 'adam',\n",
    "                  loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics= ['accuracy'])\n",
    "\n",
    "with mlflow.start_run(experiment_id = exp_id):\n",
    "    \n",
    "    mlflow.log_param('model', 'cnn')\n",
    "\n",
    "    # training model\n",
    "    start_training = time.time()\n",
    "    cnn_model.fit(X_train, y_train, epochs=5)\n",
    "    end_training = time.time()\n",
    "\n",
    "    # predict the test set\n",
    "    start_prediction = time.time()\n",
    "    y_pred_long = cnn_model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_long, axis=1)\n",
    "    end_prediction = time.time()\n",
    "\n",
    "    # predict the train set\n",
    "    y_pred_train_long = mlp_model.predict(X_train_flat)\n",
    "    y_pred_train = np.argmax(y_pred_train_long, axis=1)\n",
    "\n",
    "    # Evaluate performance\n",
    "    accuracy, precision, recall, f1_micro, f1_macro = evaluate_performance(y_test, y_pred, 'test')\n",
    "    accuracy_train, precision_train, recall_train, f1_micro_train, f1_macro_train = evaluate_performance(y_train,\n",
    "                                                                                                         y_pred_train,\n",
    "                                                                                                         'train')\n",
    "    training_time = end_training - start_training\n",
    "    prediction_time = end_prediction - start_prediction\n",
    "    print_timing(training_time, prediction_time)\n",
    "    \n",
    "    # Track performance metrics\n",
    "    track_performance_metrics(accuracy, precision, recall, f1_micro, f1_macro,\n",
    "                              accuracy_train, precision_train, recall_train, f1_micro_train, f1_macro_train,\n",
    "                              training_time, prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run MLflow tracking UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ajisamudra/anaconda3/envs/experiment-tracking/lib/python3.6/site-packages/alembic/operations/base.py:94: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  spec = inspect.getargspec(fn)\n",
      "/Users/ajisamudra/anaconda3/envs/experiment-tracking/lib/python3.6/site-packages/alembic/util/langhelpers.py:76: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  spec = inspect.getargspec(fn)\n",
      "[2020-12-11 09:59:22 +0700] [6567] [INFO] Starting gunicorn 20.0.4\n",
      "[2020-12-11 09:59:22 +0700] [6567] [INFO] Listening at: http://127.0.0.1:5000 (6567)\n",
      "[2020-12-11 09:59:22 +0700] [6567] [INFO] Using worker: sync\n",
      "[2020-12-11 09:59:22 +0700] [6570] [INFO] Booting worker with pid: 6570\n",
      "^C\n",
      "\n",
      "Aborted!\n",
      "[2020-12-11 10:01:41 +0700] [6567] [INFO] Handling signal: int\n",
      "[2020-12-11 10:01:41 +0700] [6570] [INFO] Worker exiting (pid: 6570)\n"
     ]
    }
   ],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
